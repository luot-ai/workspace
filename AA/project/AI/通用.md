* 大模型

  * LLM
    * 核心是transformer
    * BERT：使用了transformer的编码器，理解
    * GPT
    * LLaMA
  * transformer是一个encoder-decoder的结构
  * 它开销最大的部分应该是Multi-Head Self-Attention
    * 由输入X获得Q,K,V是矩阵乘
    * Q*K的转置，还有和V矩阵在乘都是矩阵乘
    * 矩阵乘计算和访存密集。可能会关注到用低精度、稀疏、fused kernel 等技术来在芯片侧做优化，可能会有一些算子和直接在底层硬件上的加速
* **吞吐率**指的是单位时间内处理的数据量，每秒几个浮点操作
* GPU

  * GPU里面它主要就是堆叠了大量的SIMT核心，然后每一个这样的核心里面会有若干的CUDA core，和共用的取值/访存部件
  * 它的调度模式是把若干个线程组成的一个线程组（一般称为warp）调度到一个SIMT核心中执行
  * 具体执行时，这个线程组会执行相同的指令序列，然后相当于在底层硬件上是以SIMD的方式执行
* TPU

  * 通过专用的脉动阵列去做
  * 比较定制化，很专一的去针对卷积去进行计算
  * 二维滑动阵列，每一个PE都可以完成一次MAC
  * 吞吐率高/能效高
    * 每个PE比较简单，只负责乘加
    * 数据重用率高，片上存储开得大，不需总是访问主存
    * 没有线程调度，warp分支的处理
    * 对shape敏感
    * 极简硬件设计
      * 没有渲染单元 硬件调度器等
* 内存架构对比

  * GPU：显存+L2cache+SM内部的shared mem/L1+寄存器
  * TPU：外部的HBM和片上的unified buffer
* 大模型和AI芯片

  * 我觉得大模型和AI芯片的关系很像发动机和燃料，
  * 大模型本身能力很强，但没有合适的计算平台，它跑不起来。
  * 现在GPT类模型这么火，其实背后是GPU/TPU这些芯片在不断演进，把训练和推理的效率拉得很高。
  * 芯片厂商也在根据模型的特点，像矩阵乘法、稀疏性、Attention机制这些，对硬件做专门优化。
  * 我对这种“软硬协同”的设计趋势特别感兴趣。
* 针对矩阵乘的优化

  * tiling
  * 稀疏
  * 低精度
* 针对Attention的优化

  * 我关注了如 FlashAttention、Sparsity、FP8 等技术，它们在解决 attention 和 matrix multiply 这两个瓶颈上效果非常显著。比如 FlashAttention 把 softmax 和 matmul 融合进一个 kernel，同时用块状访存大大减少了内存占用。这些优化都是从“访存瓶颈”这个根源入手，非常值得借鉴。如果将来能在芯片设计时把这些访存路径考虑进去，甚至为 Attention 专门做加速器设计，会非常有前景。
  * flashAttention
    * 他主要还是从访存出发，希望尽可能访问片上SRAM
    * 所以它做了三个优化点：一个是算子融合，另一个是Tiling，还有recomputation
* pytorch

  * `torch.utils.data` — 数据加载模块
  * torch.nn搭建网络结构，定义forward函数
  * torch.optim优化器
  * torch.cuda()设备迁移
  * loss.backward() optimizer.step()
* winograd局限性

  * 对带步长的卷积不友好
  * TODO
* 燧原

  * 做AI加速卡、系统集群和软硬件全栈解决方案
  * 最近做了不少面向数据中心的推理和训练加速卡
    * 燧原S60
    * 云燧i20
* 定位瓶颈

  * PyTorch Profiler + TensorBoard可以去定位和可视化执行的瓶颈
  * 然后之前写cuda会简单的使用nvprof去查看瓶颈
    * nsight system查看系统，compute进行kernel-level的分析
* Tensorrt

  * TensorRT 通过一系列的优化技术，如精度优化、内存布局优化、图优化、层融合等
  * TensorRT 接受一种标准的深度学习框架（如 TensorFlow、PyTorch 或 ONNX）的预训练模型。这些模型通常以某种标准格式（如 `ONNX` 或 `TorchScript`）表示，然后根据特定的GPU型号，生成PTX代码

岗位职责：
1. 跟踪 	前沿芯片架构/微架构设计方向，输出相关分析。
2. 分析	AI算法模型		训练推理	性能瓶颈，		与芯片设计团队共同定义	计算架构、指令和算法。
3.研究实现		AI业务场景训练推理的		加速技术，优化		异构计算pipline，提高吞吐率与延时。

任职要求：
1.硕士及以上学历（博士优先），计算机、模式识别、微电子、电子信息等相关专业。
2.掌握	Tensorflow,	Pytorch,	caffe,		mxnet,	ONNX, 	tensorrt，	mindspore	等至少一种深度学习框架或引擎。
3. 对	【深度学习】前沿算法	、芯片架构设计有强烈兴趣。
4.精通Python和C/C++，具备较强代码及工程实现能力。

小红书面经
（高性能实习）
2.对Mindspore的理解
3.算子精度和性能
4.对Block、Grid块具体怎么设置的理解**
6.对目标检测模型的了解
7.口述循环链表检测

（AI软件工程师）
对于大模型和AI芯片怎么看？怎么理解？***

（AI算子开发岗）
1、llama2模型构成结构及算子
2、A100的硬件架构
3、shared memory，warp reduce是什么
4、softmax怎么实现的，它的语义是什么
5、什么数据类型的SoftMax
6、对pytorch熟悉吗？pytorch哪些算子？**
7、使用过哪些损失函数？
8、常见激活函数
9、熟悉哪些算子的语义？
10、GPU上有几级的存储？**
11、为什么选择AI算子开发？
12、遇到问题，解决问题的过程，怎么优化的算子？
13、C++的多态性质怎么体现的？


DEEPSEEK指导
一、技术知识准备
1. 芯片架构基础
核心概念：掌握CPU/GPU/TPU架构差异、内存层次结构（缓存、带宽瓶颈）、并行计算（SIMD/SIMT/MIMD）、流水线设计、功耗优化等。

AI芯片趋势：了解燧原科技产品（如云燧T10/T11）的特性，对比TPU、Graphcore IPU等架构，思考如何优化矩阵乘、卷积等AI核心算子。

性能指标：TOPS、能效比（TOPS/W）、延迟与吞吐量权衡，如何通过微架构（如定制指令集、数据复用）优化这些指标。

2. AI模型性能分析
瓶颈识别：使用PyTorch Profiler或TensorBoard分析模型，定位计算密集型算子（如GEMM）、内存瓶颈（如激活值存储）、I/O瓶颈（数据加载）。

优化案例：准备一个实例，如将模型从FP32量化到INT8，使用TensorRT部署，说明如何提升吞吐量并保持精度。

3. 异构计算与加速技术
框架工具：熟悉TensorRT的层融合（layer fusion）、ONNX模型转换中的算子兼容性处理。

编程实践：若有CUDA/OpenCL经验，准备代码片段展示如何优化核函数（如减少全局内存访问）；若无，可讨论并行计算原理（如数据分块、共享内存使用）。

二、项目经验梳理
选材重点：选择与AI模型优化、硬件加速或芯片设计相关的项目，如：

参与过模型压缩（剪枝/量化）项目，提升推理速度。

在FPGA上实现过CNN加速器，优化数据流和计算单元。

使用Halide/TVM进行自动算子优化，降低延迟。

表述结构（STAR法则）：

情境：项目目标（如“将ResNet-50推理延迟降低50%”）。

任务：你的角色（如“负责分析瓶颈并设计量化方案”）。

行动：具体技术方法（如“使用NAS搜索最优量化位宽，插入校准层”）。

结果：量化指标提升（如“延迟降低60%，精度损失<1%”）。

三、编程能力强化
Python：熟悉numpy实现高效数据预处理，用PyTorch自定义算子（如扩展C++ CUDA算子）。

C/C++：掌握内存管理、多线程（如OpenMP）、性能优化技巧（如循环展开、SIMD指令）。

编码题准备：刷LeetCode中等难度题（动态规划、树相关），重点练习与硬件相关的题目（如实现矩阵乘法优化）。

四、面试问题预测与应答策略
技术问题示例
“如何分析一个AI模型的性能瓶颈？”

答：”首先用Profiler工具（如PyTorch Profiler）统计各算子耗时，识别计算密集型部分（如卷积）；其次分析内存占用，检查是否因激活值过大导致缓存失效；最后考虑数据加载和预处理是否阻塞计算流。“

“假设要设计一个矩阵乘法加速单元，你会考虑哪些因素？”

答：”我会优先设计数据复用策略（如利用局部性原理减少DRAM访问），其次确定并行度（如TPU的脉动阵列结构），同时考虑支持混合精度计算以提升吞吐。“

行为问题示例
“为什么选择燧原？”

答：”燧原在云端AI芯片的突破，如支持大规模集群训练，与我研究分布式训练的课题高度契合。我希望在异构计算优化方向贡献技术，同时向团队学习芯片-算法协同设计经验。“

五、公司研究与反问准备
研究燧原技术：阅读其官网技术白皮书，了解“驭算”软件栈如何支持多框架，思考如何优化框架与芯片的接口。

反问示例：

“团队目前在优化Transformer模型推理时遇到的最大挑战是什么？”

“实习生将参与架构探索的哪个具体环节？”

六、模拟面试与资源推荐
模拟练习：用“自我介绍+项目深挖+编码模拟”结构，限时回答（如2分钟自我介绍）。

学习资源：

书籍：《计算机体系结构：量化研究方法》（第6章GPU/domain-specific架构）。

论文：Google TPU系列论文（ISCA 2017）、燧原公开技术文档。

网课：Coursera《Hardware for AI》或NVIDIA CUDA编程教程。

最后提示：突出你的“桥梁”能力——既能理解算法需求，又能转化为硬件设计语言。燧原重视算法-芯片协同优化，面试中可强调两者交叉经验（如模型优化如何驱动指令集设计）。